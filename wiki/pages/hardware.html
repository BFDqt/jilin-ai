{% extends "layout.html" %}
  
{% block title %}Hardware{% endblock %}
{% block lead %}Computational Infrastructure: The High-Performance Computing Foundation Behind BIOMNIGEM.{% endblock %}

{% block page_content %}

<div class="row mt-4">
  <div class="col">
    <div class="bd-callout bd-callout-info">
      <h4>Best Hardware</h4>
      <p>This is a prize for the team that has developed a piece of hardware for synthetic biology. Hardware in iGEM should make synthetic biology based on standard parts easier, faster, better or more accessible to our community. Did your team make a sensor to help teams characterize parts? Did you make a robot that can help teams perform experiments or do cloning more easily? Strong competitors for this prize will demonstrate utility, user testing, and easy reproducibility.</p>
      <hr />
      <p>Visit the <a href="https://competition.igem.org/judging/special-prizes">Special Prizes page</a> for more information.</p>
    </div>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>Computational Hardware Infrastructure</h2>
    <hr />
    <p>Training and deploying a state-of-the-art Large Language Model like BIOMNIGEM is a computationally intensive endeavor that requires significant hardware resources. Our project would not have been possible without access to a modern high-performance computing (HPC) infrastructure.</p>
  </div>
</div>

<div class="row mt-4">
  <div class="col-lg-6">
    <h3>Training Infrastructure</h3>
    <hr />
    <div class="card">
      <div class="card-body">
        <h5 class="card-title"><i class="fas fa-microchip"></i> GPU Cluster</h5>
        <p class="card-text">The core training of our 30-billion parameter model was conducted on a distributed cluster of <strong>NVIDIA H100 and A100 Tensor Core GPUs</strong>. These are essential for handling the massive parallel computations required for both the Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages.</p>
        
        <ul>
          <li><strong>GPU Type:</strong> NVIDIA H100 & A100 Tensor Core</li>
          <li><strong>Memory per GPU:</strong> 80GB HBM2e/HBM3</li>
          <li><strong>Compute Capability:</strong> Mixed-precision training support</li>
          <li><strong>Interconnect:</strong> NVLink and InfiniBand for high-speed communication</li>
        </ul>
      </div>
    </div>
  </div>
  
  <div class="col-lg-6">
    <h3>Memory and Storage</h3>
    <hr />
    <div class="card">
      <div class="card-body">
        <h5 class="card-title"><i class="fas fa-memory"></i> High-Performance Storage</h5>
        <p class="card-text">Training required access to large amounts of high-speed RAM (over 512GB per node) to hold model parameters and intermediate data, as well as a fast, parallel file system to efficiently load our terabyte-scale biological datasets.</p>
        
        <ul>
          <li><strong>System RAM:</strong> 512GB+ DDR4/DDR5 per node</li>
          <li><strong>Storage:</strong> Parallel file system (Lustre/GPFS)</li>
          <li><strong>Dataset Size:</strong> Multi-terabyte biological datasets</li>
          <li><strong>I/O Performance:</strong> High-bandwidth data loading pipelines</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="row mt-4">
  <div class="col-lg-6">
    <h3>Inference Server</h3>
    <hr />
    <div class="card">
      <div class="card-body">
        <h5 class="card-title"><i class="fas fa-server"></i> Production Deployment</h5>
        <p class="card-text">For the live web demo and API, BIOMNIGEM is deployed on a dedicated server equipped with multiple GPUs optimized for inference. This ensures low-latency responses, allowing for real-time, interactive dialogues with researchers.</p>
        
        <ul>
          <li><strong>Inference GPUs:</strong> Multiple GPUs for concurrent users</li>
          <li><strong>Optimization:</strong> TensorRT and model quantization</li>
          <li><strong>Latency:</strong> Sub-second response times</li>
          <li><strong>Scalability:</strong> Load balancing for multiple users</li>
        </ul>
      </div>
    </div>
  </div>
  
  <div class="col-lg-6">
    <h3>Accessibility & Optimization</h3>
    <hr />
    <div class="card">
      <div class="card-body">
        <h5 class="card-title"><i class="fas fa-tools"></i> Democratizing Access</h5>
        <p class="card-text">While we utilized a powerful hardware setup, we are committed to model optimization techniques (such as quantization and distillation) to make future versions of BIOMNIGEM accessible to users with more modest hardware resources.</p>
        
        <ul>
          <li><strong>Model Quantization:</strong> 8-bit and 4-bit precision support</li>
          <li><strong>Knowledge Distillation:</strong> Smaller model variants</li>
          <li><strong>Edge Deployment:</strong> Local inference capabilities</li>
          <li><strong>Cloud API:</strong> Accessible via web interface</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h3>Hardware Specifications Summary</h3>
    <hr />
    <div class="table-responsive">
      <table class="table table-striped">
        <thead>
          <tr>
            <th>Component</th>
            <th>Specification</th>
            <th>Purpose</th>
            <th>Performance Impact</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Training GPUs</strong></td>
            <td>NVIDIA H100/A100 Cluster</td>
            <td>Model training & fine-tuning</td>
            <td>Enables 30B parameter training</td>
          </tr>
          <tr>
            <td><strong>System Memory</strong></td>
            <td>512GB+ per node</td>
            <td>Model parameter storage</td>
            <td>Supports large batch sizes</td>
          </tr>
          <tr>
            <td><strong>Storage System</strong></td>
            <td>High-speed parallel filesystem</td>
            <td>Dataset loading & checkpointing</td>
            <td>Minimizes I/O bottlenecks</td>
          </tr>
          <tr>
            <td><strong>Inference Server</strong></td>
            <td>Multi-GPU inference cluster</td>
            <td>Real-time model serving</td>
            <td>Sub-second response times</td>
          </tr>
          <tr>
            <td><strong>Network</strong></td>
            <td>High-bandwidth interconnect</td>
            <td>Multi-node communication</td>
            <td>Enables distributed training</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</div>

{% endblock %}
