{% extends "layout.html" %}
  
{% block title %}Engineering Success{% endblock %}
{% block lead %}Our Engineering Success: The Design ‚Üí Build ‚Üí Test ‚Üí Learn Cycle for BIOMNIGEM.{% endblock %}

{% block page_content %}

<div class="row mt-4">
  <div class="col">
    <div class="bd-callout bd-callout-info">
      <h4>Silver Medal Criterion #1</h4>
      <p>Demonstrate engineering success in a technical aspect of your project by going through at least one iteration of the engineering design cycle.</p>
      <hr />
      <p>Visit the <a href="https://competition.igem.org/judging/medals">Medals page</a> for more information.</p>
    </div>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>Our Engineering Success: The Design ‚Üí Build ‚Üí Test ‚Üí Learn Cycle</h2>
    <hr />
    <p>Building BIOMNIGEM required a rigorous and iterative engineering cycle. Our goal was to create a robust, multimodal AI that not only performs well on specific tasks but also understands the deep, underlying principles of biology.</p>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>üß¨ Design: Architecting a Multimodal Brain</h2>
    <hr />
    <p>Our design philosophy was to move beyond the rigid "feature extractor" models common in biology. We wanted a flexible, end-to-end system.</p>
    <ul>
      <li><strong>Core Framework:</strong> We chose <strong>Qwen3-30B</strong>, a powerful Large Language Model, as the foundational "brain" for its strong reasoning and language capabilities.</li>
      <li><strong>True Multimodal Input:</strong> We designed a unified data pipeline to convert disparate biological data types‚ÄîDNA sequences, gene expression profiles, protein sequences, and interaction networks‚Äîinto a common text-based format that the LLM could understand. For example, gene expression data was transformed into a "cell sentence" (e.g., <code>"the cell sentence is: geneA geneB geneC..."</code>).</li>
      <li><strong>Task-Driven Learning:</strong> We designed a comprehensive suite of training tasks to teach the model both single-modality and cross-modal reasoning. This forces the model to learn the connections between different biological layers.</li>
    </ul>

    <div class="table-responsive mt-3">
      <table class="table table-striped">
        <thead>
          <tr>
            <th>Task Name</th>
            <th>Type</th>
            <th>Input Information</th>
            <th>Target Output</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>DNA mutation effect prediction</strong></td>
            <td>DNA</td>
            <td>Ref. DNA sequence + mutation(s)</td>
            <td>Functional effect description</td>
          </tr>
          <tr>
            <td><strong>DNA regulatory element ID</strong></td>
            <td>DNA</td>
            <td>DNA sequence</td>
            <td>Annotated regulatory elements</td>
          </tr>
          <tr>
            <td><strong>DNA-binding protein prediction</strong></td>
            <td>DNA</td>
            <td>DNA sequence</td>
            <td>List of binding proteins</td>
          </tr>
          <tr>
            <td><strong>Protein function classification</strong></td>
            <td>Protein</td>
            <td>Protein amino acid sequence</td>
            <td>Functional category</td>
          </tr>
          <tr>
            <td><strong>Protein stability prediction</strong></td>
            <td>Protein</td>
            <td>Protein sequence ¬± mutation</td>
            <td>Stability change score</td>
          </tr>
          <tr>
            <td><strong>Protein-protein interaction pred.</strong></td>
            <td>Protein</td>
            <td>Two protein sequences</td>
            <td>Interaction probability/sites</td>
          </tr>
          <tr>
            <td><strong>Gene ‚Üí protein structure pred.</strong></td>
            <td>Cross-modal</td>
            <td>Coding sequence + context</td>
            <td>3D protein structure</td>
          </tr>
          <tr>
            <td><strong>Gene expression ‚Üí protein func.</strong></td>
            <td>Cross-modal</td>
            <td>Gene expression profile</td>
            <td>Protein functional activity</td>
          </tr>
          <tr>
            <td><strong>DNA + protein ‚Üí pathway inference</strong></td>
            <td>Cross-modal</td>
            <td>DNA variants + protein data</td>
            <td>Altered pathway map</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>üõ†Ô∏è Build: The Three-Stage Training Pipeline</h2>
    <hr />
    <p>With the architecture designed, we built the model through a sophisticated, three-stage training process.</p>
    
    <h4>1. Supervised Fine-Tuning (SFT):</h4>
    <p>We first taught BIOMNIGEM the fundamentals. We fine-tuned the base Qwen3 model on a curated corpus containing biological textbooks, research papers, and structured task data. This gave it the foundational knowledge of a biologist.</p>
    
    <h4>2. Reinforcement Learning (RL) for Performance:</h4>
    <p>To sharpen its predictive accuracy, we used an advanced algorithm called <strong>GRPO (Ghost Reward Policy Optimization)</strong>. We rewarded the model for correctness on specific tasks, using metrics like accuracy for classification and <strong>BioBERTScore</strong> (a measure of semantic similarity in biology) for generative answers.</p>
    
    <h4>3. Instruction & Dialogue Tuning for Reasoning:</h4>
    <p>In the final stage, we used GRPO again, but this time to train the model on open-ended questions, multi-turn dialogues, and reasoning tasks. This is what transformed BIOMNIGEM from a simple predictor into a collaborative expert capable of "Biological Chain-of-Thought" (BioCoT).</p>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>üß™ Test: Proving Superior Performance</h2>
    <hr />
    <p>The ultimate test is performance. We benchmarked BIOMNIGEM against massive, general-purpose commercial models on a real-world biological task: cell type annotation from a gene expression profile.</p>
    
    <h4>Cell Type Annotation Accuracy (humanPBMC dataset):</h4>
    <ul>
      <li>LLaMA-3.1-70B: 77%</li>
      <li>GPT-4o: 89%</li>
      <li><strong>BIOMNIGEM (SFT-trained): 93%</strong></li>
      <li><strong>BIOMNIGEM (RL-trained): 97%</strong></li>
    </ul>
    
    <p>Our results were definitive. Even with fewer parameters, our specialized, multimodally-trained model significantly outperformed the largest commercial models, demonstrating the power of our approach.</p>
  </div>
</div>

<div class="row mt-4">
  <div class="col">
    <h2>üß† Learn: Validating Our Hypothesis</h2>
    <hr />
    <p>Our engineering cycle proved our core hypothesis: <strong>true multimodal training enhances performance</strong>. By learning the relationships between different data types, BIOMNIGEM develops a more robust and accurate understanding of biology as a whole. This approach not only boosts performance on single-modality tasks but also unlocks the ability to solve complex, multi-step problems that were previously intractable. The success of this cycle paves the way for a new generation of AI in synthetic biology.</p>
  </div>
</div>

{% endblock %}
